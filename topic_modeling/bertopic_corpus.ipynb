{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install protobuf # für NLP/swissbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from bertopic.representation import PartOfSpeech\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.backend import BaseEmbedder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komplettes Korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Englishcsv_file_path = '/Users/davideventre/Desktop/telegram_daten/subcorpus_tp1.csv'\n",
    "# Read the CSV file\n",
    "# wort = input(\"Enter the name of the variable: \")\n",
    "# Name of the output folder and the file\n",
    "teilprojekt = 'tp1'\n",
    "output_folder_name = \"tp1_solidaritaet\"\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_name):\n",
    "    os.makedirs(output_folder_name)\n",
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()\n",
    "filename = \"/Users/davideventre/Desktop/telegram_daten/sampled_subc_tp1.csv\"\n",
    "csv_file_path = os.path.join(current_working_directory, filename)\n",
    "df = pd.read_csv(csv_file_path, sep='\\t', low_memory=False)\n",
    "# Get the number of rows in the DataFrame\n",
    "num_rows = len(df)\n",
    "korpus_content = df['text_content'].astype(str).tolist()\n",
    "korpus_text_date = df['text_date'].astype(str).tolist()\n",
    "korpus_text_source = df['text_source'].astype(str).tolist()\n",
    "korpus_text_id = df['text_id'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeitraum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721\n",
      "721\n",
      "721\n",
      "721\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to parse dates\n",
    "def parse_date(date_str):\n",
    "    for fmt in ('%d-%m-%Y', '%Y-%m'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "# Define your time frames as a dictionary with corpus names as keys\n",
    "time_frames = {\n",
    "    'korpus1': (parse_date('01-01-2020'), parse_date('01-01-2021')),\n",
    "}\n",
    "\n",
    "# Initialize your sentences_corpus dictionary\n",
    "korpus_content = []\n",
    "korpus_text_date = []\n",
    "korpus_text_source = []\n",
    "korpus_text_id = []\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"/Users/davideventre/Desktop/telegram_daten/sampled_subc_tp1.csv\"  # Update with the actual file path\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')  # Replace with your actual file path\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        channel_date = parse_date(row['text_date'])\n",
    "    except ValueError:\n",
    "        continue  # Skip rows with invalid date formats\n",
    "\n",
    "    # Check each corpus defined in time_frames\n",
    "    for corpus_name, (start_date, end_date) in time_frames.items():\n",
    "        if start_date <= channel_date <= end_date:\n",
    "            korpus_content.append(row['text_content'])\n",
    "            korpus_text_date.append(row['text_date'])\n",
    "            korpus_text_source.append(row['text_source'])\n",
    "            korpus_text_id.append(row['text_id'])\n",
    "\n",
    "print(len(korpus_content))\n",
    "print(len(korpus_text_date))\n",
    "print(len(korpus_text_source))\n",
    "print(len(korpus_text_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23136\n",
      "23136\n",
      "23136\n",
      "23136\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"/Users/davideventre/Desktop/telegram_daten/sampled_subc_tp1.csv\"  # Update with the actual file path\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')\n",
    "\n",
    "# Define search words for each corpus as a list\n",
    "corpora = ['faz', 'BILD']  # Add as many as needed\n",
    "\n",
    "# Initialize your sentences_corpus dictionary\n",
    "korpus_content = []\n",
    "korpus_text_date = []\n",
    "korpus_text_source = []\n",
    "korpus_text_id = []\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')  # Replace with your actual file path\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    channel_id = row['text_source']\n",
    "    message = row['text_content'] \n",
    "    for corpus in corpora:\n",
    "        if corpus in channel_id:\n",
    "            korpus_content.append(row['text_content'])\n",
    "            korpus_text_date.append(row['text_date'])\n",
    "            korpus_text_source.append(row['text_source'])\n",
    "            korpus_text_id.append(row['text_id'])\n",
    "            \n",
    "print(len(korpus_content))\n",
    "print(len(korpus_text_date))\n",
    "print(len(korpus_text_source))\n",
    "print(len(korpus_text_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "#embedding_model = pipeline(\"feature-extraction\", model=\"bert-base-german-cased\")\n",
    "#embedding_model = pipeline(model=\"ZurichNLP/swissbert\")\n",
    "#embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "# n_neighbors: höhere Werte nehmen eine \"globalere\" Perspektive der Embeddings ein (grössere Cluster)\n",
    "# n_neighbors: tiefere Werte nehmen eine \"lokalere\" Perspektive der Embeddings ein\n",
    "# n_components: tiefere Werte beeinflussen die Qualität der Embeddings\n",
    "# n_components: hohe Werte dauern länger und HDBScan braucht länger für die Berechnung\n",
    "\n",
    "dim_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine')\n",
    "\n",
    "\n",
    "# Eine schnellere Art die Dimensionen zu reduzieren\n",
    "#dim_model = PCA(n_components=5)\n",
    "#topic_model = BERTopic(umap_model=dim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# min_cluster_size: wie gross die Cluster mindestens sein müssen\n",
    "# min_samples: die Zahl der Outlier, tiefere Zahlen reduzieren die Outlier\n",
    "cluster_model = HDBSCAN(min_cluster_size=25, metric='euclidean', \n",
    "                        cluster_selection_method='eom', prediction_data=True, min_samples=3)\n",
    "\n",
    "# andere Art des Clustering, das keine Ausreisser produziert\n",
    "#cluster_model = KMeans(n_clusters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# min_df: wie oft ein Wort vorkommen muss, bevor es in die Repräsentation gelangt\n",
    "# so kann man bei grossen Dokumenten die Berechnung verkürzen\n",
    "# ngram_range: bestimmt die Länge der Ngrams, die in der Repräsentation erscheinen\n",
    "vectorizer_model = CountVectorizer(min_df=1, ngram_range=(1, 3))\n",
    "\n",
    "# max_features: topic term matrix wird kontrolliert. anstelle das man min_df einstellen muss\n",
    "#vectorizer_model = CountVectorizer(max_features=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# man kann folgende Parameter einfügen: \n",
    "# reduce_frequent_words oder BM25\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "# besser mit Stoppwörtern:\n",
    "#ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)\n",
    "#ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verschiedene Repräsentationsmöglichkeiten:\n",
    "# Keywords\n",
    "representation_model = [KeyBERTInspired(top_n_words=10), MaximalMarginalRelevance(diversity=0.95)]\n",
    "# Sprachliche Muster\n",
    "#pos_patterns = [[{'POS': 'ADJ'}, {'POS': 'NOUN'}], [{'POS': 'NOUN'}], [{'POS': 'ADJ'}]]\n",
    "#representation_model = PartOfSpeech(\"de_core_web_sm\", pos_patterns=pos_patterns)\n",
    "# möglichst unterschiedliche Wörter\n",
    "#representation_model = MaximalMarginalRelevance(diversity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausführung BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISMTOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "TOKENIZERS_PARALLELISMhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable \t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable =(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x1ab6dabb0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = BERTopic(embedding_model=embedding_model, representation_model=representation_model, umap_model=dim_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model)\n",
    "topic_model.fit(korpus_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "topics, probs = topic_model.fit_transform(korpus_content)\n",
    "topic_model_distr = BERTopic().fit(korpus_content)\n",
    "# Reduce outliers\n",
    "new_topics = topic_model.reduce_outliers(korpus_content, topics)\n",
    "# versions of dependencies and python used. loading and saving model => same dependencies and python\n",
    "# saved in one version of Bertopic should not be loaded in others\n",
    "#topic_model = BERTopic.load(\"meditopic_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übersicht Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_topics_html(model, output_folder_name, m, text_content, text_id, text_date, text_source):\n",
    "    meta_data_id = {}\n",
    "    meta_data_id['text_id'] = text_id\n",
    "    meta_data_id['text_date'] = text_date\n",
    "    meta_data_id['text_source'] = text_source\n",
    "    topic_info = model.get_document_info(text_content, metadata=meta_data_id)\n",
    "    # Assuming topic_info is a list of dictionaries with keys like 'document_id', 'topic_distribution', etc.\n",
    "    # Saving the data to a .txt file\n",
    "    topic_data = pd.DataFrame(topic_info)\n",
    "    csv_file_path1 = os.path.join(output_folder_name, m + 'list_' + '.csv')\n",
    "    topic_data.to_csv(csv_file_path1, index=False)  # Set index=False to exclude row indices in the output\n",
    "\n",
    "visualize_and_save_topics_html(topic_model, output_folder_name, teilprojekt, korpus_content, korpus_text_id, korpus_text_date, korpus_text_source)#, korpus_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_topics_html(model, output_folder_name, m):\n",
    "    fig = model.visualize_topics()\n",
    "    html = pio.to_html(fig)\n",
    "    html_file_path = os.path.join(output_folder_name, m + '_topic' + '.html')\n",
    "    with open(html_file_path, 'w') as f:\n",
    "        f.write(html)\n",
    "visualize_and_save_topics_html(topic_model, output_folder_name, teilprojekt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_barchart_topics(model, output_folder_name, m):\n",
    "    fig0 = model.visualize_barchart(top_n_topics=20, n_words=20)\n",
    "    # Convert the figure to HTML\n",
    "    html0 = pio.to_html(fig0)\n",
    "    html_file_path0 = os.path.join(output_folder_name, m + '_barchart' + '.html')\n",
    "    with open(html_file_path0, 'w') as f:\n",
    "        f.write(html0)\n",
    "    \n",
    "\n",
    "visualize_and_save_barchart_topics(topic_model, output_folder_name, teilprojekt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize and save heatmap\n",
    "def visualize_and_save_heatmap(model, output_folder_name, m):\n",
    "    heatmap = model.visualize_heatmap()\n",
    "    html4 = pio.to_html(heatmap)\n",
    "    html_file_path3 = os.path.join(output_folder_name, m + '_heatmap' + '.html')\n",
    "    with open(html_file_path3, 'w') as f:\n",
    "        f.write(html4)\n",
    "\n",
    "visualize_and_save_heatmap(topic_model, output_folder_name, teilprojekt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [49:23<00:00, 23.71s/it]\n"
     ]
    }
   ],
   "source": [
    "def visualize_and_save_hierarchical_topics(model, output_folder_name, m, korpus_content):\n",
    "    hierarchical_topics = model.hierarchical_topics(korpus_content)\n",
    "    fig2 = model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "    html2 = pio.to_html(fig2)\n",
    "    html_file_path2 = os.path.join(output_folder_name, m + '_hierarchical' + '.html')\n",
    "    with open(html_file_path2, 'w') as f:\n",
    "        f.write(html2)\n",
    "    \n",
    "visualize_and_save_hierarchical_topics(topic_model, output_folder_name, teilprojekt, korpus_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError\n"
     ]
    }
   ],
   "source": [
    "def visualize_clusters(model, output_folder_name, m, korpus_content, text_date):\n",
    "    try:\n",
    "        topics_over_time = model.topics_over_time(korpus_content, text_date, nr_bins=10)\n",
    "        fig1 = model.visualize_topics_over_time(topics_over_time, top_n_topics=100)\n",
    "        html1 = pio.to_html(fig1)\n",
    "        html_file_path1 = os.path.join(output_folder_name, m + 'dynamic' + '.html')\n",
    "        with open(html_file_path1, 'w') as f:\n",
    "            f.write(html1)\n",
    "    except ValueError:\n",
    "        print('ValueError')\n",
    "        pass\n",
    "visualize_clusters(topic_model, output_folder_name, teilprojekt, korpus_content, korpus_text_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 723/723 [12:40<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "korpus_embeddings = embedding_model.encode(korpus_content, show_progress_bar=True)\n",
    "def visualize_clusters(model, output_folder_name, m, korpus_content, embeddings):\n",
    "    try:\n",
    "        visualize_docs = model.visualize_documents(korpus_content, embeddings=embeddings, hide_annotations=True, hide_document_hover=True, width=2400, height=1400)\n",
    "        html_file_path7 = os.path.join(output_folder_name, m + 'documents' + '.html')\n",
    "        html3 = pio.to_html(visualize_docs)\n",
    "        with open(html_file_path7, 'w') as f:\n",
    "            f.write(html3)\n",
    "    except ValueError:\n",
    "        print('ValueError')\n",
    "        pass\n",
    "    # Get document info\n",
    "    \n",
    "visualize_clusters(topic_model, output_folder_name, teilprojekt, korpus_content, korpus_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(model, output_folder_name, m, korpus_content, classes):\n",
    "    # Topics ordered by a specific class \n",
    "    try:\n",
    "        topics_per_class = model.topics_per_class(korpus_content, classes=classes)\n",
    "        visualize_class = model.visualize_topics_per_class(topics_per_class, top_n_topics=10)\n",
    "        html5 = pio.to_html(visualize_class)\n",
    "        html_file_path5 = os.path.join(output_folder_name, m + 'class' + '.html')\n",
    "        with open(html_file_path5, 'w') as f:\n",
    "            f.write(html5)\n",
    "    except ValueError:\n",
    "        print(f'inconsistent shapes {m}')\n",
    "visualize_clusters(topic_model, output_folder_name, teilprojekt, korpus_content, korpus_text_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Topics in Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m visualize_class \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mvisualize_distribution(topic_distr[\u001b[38;5;241m233\u001b[39m])\n\u001b[1;32m      3\u001b[0m html5 \u001b[38;5;241m=\u001b[39m pio\u001b[38;5;241m.\u001b[39mto_html(visualize_class)\n\u001b[0;32m----> 4\u001b[0m html_file_path5 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder_name, \u001b[43mm\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistribution\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(html_file_path5, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(html5)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "def visualize_clusters(model, output_folder_name, m, text_content, classes):\n",
    "    # Topics ordered by a specific class \n",
    "    try:\n",
    "        topic_distr, _ = model.approximate_distribution(text_content)\n",
    "        visualize_class = model.visualize_distribution(topic_distr[0])\n",
    "        html5 = pio.to_html(visualize_class)\n",
    "        html_file_path5 = os.path.join(output_folder_name, 'distribution' + '.html')\n",
    "        with open(html_file_path5, 'w') as f:\n",
    "            f.write(html5)\n",
    "    except ValueError:\n",
    "        print(f'inconsistent shapes {m}')\n",
    "visualize_clusters(topic_model, output_folder_name, \"korpus\", korpus_content, korpus_text_source)#, korpus1_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
