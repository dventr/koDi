{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davideventre/Desktop/bertopic_kodi_2/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from bertopic.representation import PartOfSpeech\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, OpenAI, PartOfSpeech\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.backend import BaseEmbedder\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the output folder and the file\n",
    "output_folder_name = \"tp1_compare\"\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_name):\n",
    "    os.makedirs(output_folder_name)\n",
    "# Get the current working directory\n",
    "current_working_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "#embedding_model = pipeline(\"feature-extraction\", model=\"bert-base-german-cased\")\n",
    "#embedding_model = pipeline(model=\"ZurichNLP/swissbert\")\n",
    "#embedding_model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "# n_neighbors: höhere Werte nehmen eine \"globalere\" Perspektive der Embeddings ein (grössere Cluster)\n",
    "# n_neighbors: tiefere Werte nehmen eine \"lokalere\" Perspektive der Embeddings ein\n",
    "# n_components: tiefere Werte beeinflussen die Qualität der Embeddings\n",
    "# n_components: hohe Werte dauern länger und HDBScan braucht länger für die Berechnung\n",
    "\n",
    "dim_model = UMAP(n_neighbors=15, n_components=10, min_dist=0.0, metric='cosine')\n",
    "\n",
    "\n",
    "# Eine schnellere Art die Dimensionen zu reduzieren\n",
    "#dim_model = PCA(n_components=5)\n",
    "#topic_model = BERTopic(umap_model=dim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# min_cluster_size: wie gross die Cluster mindestens sein müssen\n",
    "# min_samples: die Zahl der Outlier, tiefere Zahlen reduzieren die Outlier\n",
    "cluster_model = HDBSCAN(min_cluster_size=25, metric='euclidean', \n",
    "                        cluster_selection_method='eom', prediction_data=True, min_samples=3)\n",
    "\n",
    "# andere Art des Clustering, das keine Ausreisser produziert\n",
    "#cluster_model = KMeans(n_clusters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# min_df: wie oft ein Wort vorkommen muss, bevor es in die Repräsentation gelangt\n",
    "# so kann man bei grossen Dokumenten die Berechnung verkürzen\n",
    "# ngram_range: bestimmt die Länge der Ngrams, die in der Repräsentation erscheinen\n",
    "vectorizer_model = CountVectorizer(min_df=1, ngram_range=(1, 3))\n",
    "\n",
    "# max_features: topic term matrix wird kontrolliert. anstelle das man min_df einstellen muss\n",
    "#vectorizer_model = CountVectorizer(max_features=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# man kann folgende Parameter einfügen: \n",
    "# reduce_frequent_words oder BM25\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "# besser mit Stoppwörtern:\n",
    "#ctfidf_model = ClassTfidfTransformer(bm25_weighting=True)\n",
    "#ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verschiedene Repräsentationsmöglichkeiten:\n",
    "# Keywords\n",
    "representation_model = [KeyBERTInspired(top_n_words=10), MaximalMarginalRelevance(diversity=0.95)]\n",
    "# Sprachliche Muster\n",
    "#pos_patterns = [[{'POS': 'ADJ'}, {'POS': 'NOUN'}], [{'POS': 'NOUN'}], [{'POS': 'ADJ'}]]\n",
    "#representation_model = PartOfSpeech(\"de_core_web_sm\", pos_patterns=pos_patterns)\n",
    "# möglichst unterschiedliche Wörter\n",
    "#representation_model = MaximalMarginalRelevance(diversity=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwei unterschiedliche Quellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'channel_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Loop durch Quellen\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m corpus \u001b[38;5;129;01min\u001b[39;00m quelle:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m corpus \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchannel_id\u001b[49m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# Liste für Korpus 1\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m corpora[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m corpus:\n\u001b[1;32m     30\u001b[0m             korpus1_content\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_content\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'channel_id' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Path zu der .csv Datei\n",
    "csv_file_path = \"./sampled_subc_tp1.csv\"  \n",
    "# .csv Datei einlesen\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')\n",
    "\n",
    "# Quellen, die man miteinander vergleichen möchte\n",
    "corpora = ['faz', 'BILD']  # nur zwei Quellen als Eingabemöglichkeit\n",
    "\n",
    "# Listen für Korpus 1 (Position 1 in Liste)\n",
    "korpus1_content = []\n",
    "korpus1_text_date = []\n",
    "korpus1_text_source = []\n",
    "korpus1_text_id = []\n",
    "\n",
    "# Listen für Korpus 2 (Position 2 in Liste)\n",
    "korpus2_content = []\n",
    "korpus2_text_date = []\n",
    "korpus2_text_source = []\n",
    "korpus2_text_id = []\n",
    "\n",
    "# Loop durch alle Reihen in der .csv Datei\n",
    "for index, row in df.iterrows():\n",
    "    quelle = row['text_source']\n",
    "    # Loop durch Quellen\n",
    "    for corpus in quelle:\n",
    "        if corpus in channel_id:\n",
    "            # Liste für Korpus 1\n",
    "            if corpora[0] == corpus:\n",
    "                korpus1_content.append(row['text_content'])\n",
    "                korpus1_text_date.append(row['text_date'])\n",
    "                korpus1_text_source.append(row['text_source'])\n",
    "                korpus1_text_id.append(row['text_id'])\n",
    "            # Liste für Korpus 2\n",
    "            elif corpora[1] == corpus:\n",
    "                korpus2_content.append(row['text_content'])\n",
    "                korpus2_text_date.append(row['text_date'])\n",
    "                korpus2_text_source.append(row['text_source'])\n",
    "                korpus2_text_id.append(row['text_id'])\n",
    "                \n",
    "# Kontrolle der Listenlänge: alle Listen müssen gleich lang sein         \n",
    "print(len(korpus1_content))\n",
    "print(len(korpus1_text_date))\n",
    "print(len(korpus1_text_source))\n",
    "print(len(korpus1_text_id))\n",
    "\n",
    "print(len(korpus2_content))\n",
    "print(len(korpus2_text_date))\n",
    "print(len(korpus2_text_source))\n",
    "print(len(korpus2_text_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwei unterschiedliche Zeiträume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "721\n",
      "721\n",
      "721\n",
      "721\n",
      "663\n",
      "663\n",
      "663\n",
      "663\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Path zur .csv Datei\n",
    "csv_file_path = \"./sampled_subc_tp1.csv\"  # neuer File Path erstellen\n",
    "# .csv Datei einlesen\n",
    "df = pd.read_csv(csv_file_path, sep='\\t') \n",
    "# Funktion um Datum zu parsen\n",
    "def parse_date(date_str):\n",
    "    for fmt in ('%d-%m-%Y', '%Y-%m'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "# Define your time frames as a dictionary with corpus names as keys\n",
    "time_frames = {\n",
    "    'korpus1': (parse_date('01-01-2020'), parse_date('01-01-2021')),\n",
    "    'korpus2': (parse_date('01-01-2019'), parse_date('01-01-2020')),\n",
    "}\n",
    "\n",
    "# Initialize your sentences_corpus dictionary\n",
    "korpus1_content = []\n",
    "korpus1_text_date = []\n",
    "korpus1_text_source = []\n",
    "korpus1_text_id = []\n",
    "\n",
    "korpus2_content = []\n",
    "korpus2_text_date = []\n",
    "korpus2_text_source = []\n",
    "korpus2_text_id = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        channel_date = parse_date(row['text_date'])\n",
    "    except ValueError:\n",
    "        continue  # Skip rows with invalid date formats\n",
    "\n",
    "    # Check each corpus defined in time_frames\n",
    "    for corpus_name, (start_date, end_date) in time_frames.items():\n",
    "        if start_date <= channel_date <= end_date:\n",
    "            if corpus_name == 'korpus1':\n",
    "                korpus1_content.append(row['text_content'])\n",
    "                korpus1_text_date.append(row['text_date'])\n",
    "                korpus1_text_source.append(row['text_source'])\n",
    "                korpus1_text_id.append(row['text_id'])\n",
    "            elif corpus_name == 'korpus2':\n",
    "                korpus2_content.append(row['text_content'])\n",
    "                korpus2_text_date.append(row['text_date'])\n",
    "                korpus2_text_source.append(row['text_source'])\n",
    "                korpus2_text_id.append(row['text_id'])\n",
    "\n",
    "\n",
    "print(len(korpus1_content))\n",
    "print(len(korpus1_text_date))\n",
    "print(len(korpus1_text_source))\n",
    "print(len(korpus1_text_id))\n",
    "\n",
    "print(len(korpus2_content))\n",
    "print(len(korpus2_text_date))\n",
    "print(len(korpus2_text_source))\n",
    "print(len(korpus2_text_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwei Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x17e5b7f70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"./sampled_subc_tp1.csv\"\n",
    "csv_file_path = os.path.join(current_working_directory, filename)\n",
    "\n",
    "df = pd.read_csv(csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "korpus1_content = df['text_content'].astype(str).tolist()\n",
    "korpus1_text_date = df['text_date'].astype(str).tolist()\n",
    "korpus1_text_source = df['text_source'].astype(str).tolist()\n",
    "korpus1_text_id = df['text_id'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x17e5d7f70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename2 = \"./modularity_sampled_telegram_data.csv\"\n",
    "csv_file_path2 = os.path.join(current_working_directory, filename2)\n",
    "df_2 = pd.read_csv(csv_file_path2, sep=',', low_memory=False)\n",
    "\n",
    "korpus2_content = df_2['message'].astype(str).tolist()\n",
    "korpus2_text_date = df_2['date'].astype(str).tolist()\n",
    "korpus2_text_source = df_2['mapped_number'].astype(str).tolist()\n",
    "korpus2_text_source2 = df_2['channel_id'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x170b242b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus1_model = BERTopic(embedding_model=embedding_model, representation_model=representation_model, umap_model=dim_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model)\n",
    "korpus1_model.fit(korpus1_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x170b240a0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korpus2_model = BERTopic(embedding_model=embedding_model, representation_model=representation_model, umap_model=dim_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model)\n",
    "korpus2_model.fit(korpus2_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp1_compare/compare.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(korpus1_model.topic_embeddings_, korpus2_model.topic_embeddings_)\n",
    "\n",
    "import csv\n",
    "# Define the file name for your CSV file\n",
    "csv_file_path_compare = os.path.join(output_folder_name, 'compare' + '.csv')\n",
    "print(csv_file_path_compare)\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file_path_compare, mode='w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    # Write a header row to the CSV file (optional)\n",
    "    csv_writer.writerow([\"Media Number\", \"Representation Media Topic\", \"Telegram Number\", \"Representation Media Topic\"])\n",
    "    # Initialize the topic variable\n",
    "    topic = 0\n",
    "    x = 0\n",
    "    topic_dict_en = {}\n",
    "    topic_dict_nl = {}\n",
    "    # Loop 100 times\n",
    "    for i in range(200):\n",
    "        # Get the topics\n",
    "        korpus1_topic = korpus1_model.get_topic(topic)\n",
    "        korpus1_topic2 = korpus1_model.get_topic_info(topic)\n",
    "        strings = korpus1_topic2['Topic']\n",
    "        strings = str(strings).split('\\n')\n",
    "        strings =strings[0].split(' ')\n",
    "        try:\n",
    "            most_similar_topic_index = np.argmax(sim_matrix[topic + 1]) - 1\n",
    "            korpus2_topic = korpus2_model.get_topic(most_similar_topic_index)\n",
    "            korpus2_topic2 = korpus2_model.get_topic_info(most_similar_topic_index)\n",
    "            strings2 = korpus2_topic2['Topic']\n",
    "            strings2 = str(strings2).split('\\n')\n",
    "            strings2 = strings2[0].split(' ')\n",
    "            if len(strings2) <= 4:\n",
    "                strings2.append(strings2[3])\n",
    "        except IndexError:\n",
    "            break\n",
    "        korpus1_topic = str(korpus1_topic).replace(\"[\", \"\")\n",
    "        korpus1_topic = korpus1_topic.replace(\"]\", \"\")\n",
    "        korpus1_topic = korpus1_topic.replace(\"(\", \"\")\n",
    "        korpus1_topic = korpus1_topic.replace('\"', \"\")\n",
    "        korpus1_topic = korpus1_topic.replace(')\"', \"\")\n",
    "\n",
    "        korpus2_topic = str(korpus2_topic).replace(\"[\", \"\")\n",
    "        korpus2_topic = korpus2_topic.replace(\"]\", \"\")\n",
    "        korpus2_topic = korpus2_topic.replace(\"(\", \"\")\n",
    "        korpus2_topic = korpus2_topic.replace('\"', \"\")\n",
    "        korpus2_topic = korpus2_topic.replace(')\"', \"\")\n",
    "\n",
    "\n",
    "        # Write the topics to the CSV file\n",
    "        csv_writer.writerow([strings[4], korpus1_topic.replace(\"),\", \"\\n\"), strings2[4], korpus2_topic.replace(\"),\", \"\\n\")])\n",
    "        # Increment the topic variable\n",
    "        topic += 1\n",
    "        x += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_topics_html(model, output_folder_name, m, text_content, text_id, text_date, text_source):\n",
    "    meta_data_id = {}\n",
    "    meta_data_id['text_id'] = text_id\n",
    "    meta_data_id['text_date'] = text_date\n",
    "    meta_data_id['text_source'] = text_source\n",
    "    topic_info = model.get_document_info(text_content, metadata=meta_data_id)\n",
    "    # Assuming topic_info is a list of dictionaries with keys like 'document_id', 'topic_distribution', etc.\n",
    "    # Saving the data to a .txt file\n",
    "    topic_data = pd.DataFrame(topic_info)\n",
    "    csv_file_path1 = os.path.join(output_folder_name, m + 'list_' + '.csv')\n",
    "    topic_data.to_csv(csv_file_path1, index=False)  # Set index=False to exclude row indices in the output\n",
    "\n",
    "visualize_and_save_topics_html(korpus1_model, output_folder_name, \"korpus1\", korpus1_content, korpus1_text_id, korpus1_text_date, korpus1_text_source)#, korpus1_embeddings)\n",
    "visualize_and_save_topics_html(korpus2_model, output_folder_name, \"korpus2\", korpus2_content, korpus2_text_source, korpus2_text_date, korpus2_text_source)#, korpus2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_topics_html(model, output_folder_name, m):\n",
    "    fig = model.visualize_topics()\n",
    "    html = pio.to_html(fig)\n",
    "    html_file_path = os.path.join(output_folder_name, m + '_topic' + '.html')\n",
    "    with open(html_file_path, 'w') as f:\n",
    "        f.write(html)\n",
    "visualize_and_save_topics_html(korpus1_model, output_folder_name, \"korpus1\")#, korpus1_embeddings)\n",
    "visualize_and_save_topics_html(korpus2_model, output_folder_name, \"korpus2\")#, korpus2_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_barchart_topics(model, output_folder_name, m):\n",
    "    fig0 = model.visualize_barchart(top_n_topics=20, n_words=20)\n",
    "    # Convert the figure to HTML\n",
    "    html0 = pio.to_html(fig0)\n",
    "    html_file_path0 = os.path.join(output_folder_name, m + '_barchart' + '.html')\n",
    "    with open(html_file_path0, 'w') as f:\n",
    "        f.write(html0)\n",
    "    \n",
    "\n",
    "visualize_and_save_barchart_topics(korpus1_model, output_folder_name, \"korpus1\")#, korpus1_embeddings)\n",
    "visualize_and_save_barchart_topics(korpus2_model, output_folder_name, \"korpus2\")#, korpus2_embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize and save heatmap\n",
    "def visualize_and_save_heatmap(model, output_folder_name, m):\n",
    "    heatmap = model.visualize_heatmap()\n",
    "    html4 = pio.to_html(heatmap)\n",
    "    html_file_path3 = os.path.join(output_folder_name, m + '_heatmap' + '.html')\n",
    "    with open(html_file_path3, 'w') as f:\n",
    "        f.write(html4)\n",
    "\n",
    "visualize_and_save_heatmap(korpus1_model, output_folder_name, \"korpus1\")#, korpus1_embeddings)\n",
    "visualize_and_save_heatmap(korpus2_model, output_folder_name, \"korpus2\")#, korpus2_embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:15<00:00,  1.70s/it]\n",
      "100%|██████████| 9/9 [00:16<00:00,  1.85s/it]\n"
     ]
    }
   ],
   "source": [
    "def visualize_and_save_hierarchical_topics(model, output_folder_name, m, text_content):\n",
    "    hierarchical_topics = model.hierarchical_topics(text_content)\n",
    "    fig2 = model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "    html2 = pio.to_html(fig2)\n",
    "    html_file_path2 = os.path.join(output_folder_name, m + '_hierarchical' + '.html')\n",
    "    with open(html_file_path2, 'w') as f:\n",
    "        f.write(html2)\n",
    "    \n",
    "visualize_and_save_hierarchical_topics(korpus1_model, output_folder_name, \"korpus1\", korpus1_content)#, korpus1_embeddings)\n",
    "visualize_and_save_hierarchical_topics(korpus2_model, output_folder_name, \"korpus2\", korpus2_content)#, korpus2_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError\n"
     ]
    }
   ],
   "source": [
    "def visualize_clusters(model, output_folder_name, m, text_content, text_date):\n",
    "    try:\n",
    "        topics_over_time = model.topics_over_time(text_content, text_date, nr_bins=10)\n",
    "        fig1 = model.visualize_topics_over_time(topics_over_time, top_n_topics=100)\n",
    "        html1 = pio.to_html(fig1)\n",
    "        html_file_path1 = os.path.join(output_folder_name, m + 'dynamic' + '.html')\n",
    "        with open(html_file_path1, 'w') as f:\n",
    "            f.write(html1)\n",
    "    except ValueError:\n",
    "        print('ValueError')\n",
    "        pass\n",
    "visualize_clusters(korpus1_model, output_folder_name, \"korpus1\", korpus1_content, korpus1_text_date)#, korpus1_embeddings)\n",
    "visualize_clusters(korpus2_model, output_folder_name, \"korpus2\", korpus2_content, korpus2_text_date)#, korpus2_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 23/23 [00:24<00:00,  1.06s/it]\n",
      "Batches: 100%|██████████| 21/21 [00:21<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "korpus1_embeddings = embedding_model.encode(korpus1_content, show_progress_bar=True)\n",
    "korpus2_embeddings = embedding_model.encode(korpus2_content, show_progress_bar=True)\n",
    "def visualize_clusters(model, output_folder_name, m, text_content, embeddings):\n",
    "    try:\n",
    "        visualize_docs = model.visualize_documents(text_content, embeddings=embeddings, hide_annotations=True, hide_document_hover=True, width=2400, height=1400)\n",
    "        html_file_path7 = os.path.join(output_folder_name, m + 'documents_' + '.html')\n",
    "        html3 = pio.to_html(visualize_docs)\n",
    "        with open(html_file_path7, 'w') as f:\n",
    "            f.write(html3)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # Get document info\n",
    "    \n",
    "visualize_clusters(korpus1_model, output_folder_name, \"korpus1\", korpus1_content, korpus1_embeddings)#, korpus1_embeddings)\n",
    "visualize_clusters(korpus2_model, output_folder_name, \"korpus2\", korpus2_content, korpus2_embeddings)#, korpus2_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistent shapes korpus1\n",
      "inconsistent shapes korpus1\n"
     ]
    }
   ],
   "source": [
    "def visualize_clusters(model, output_folder_name, m, text_content, classes):\n",
    "    # Topics ordered by a specific class \n",
    "    try:\n",
    "        topics_per_class = model.topics_per_class(text_content, classes=classes)\n",
    "        visualize_class = model.visualize_topics_per_class(topics_per_class, top_n_topics=10)\n",
    "        html5 = pio.to_html(visualize_class)\n",
    "        html_file_path5 = os.path.join(output_folder_name, 'class' + '.html')\n",
    "        with open(html_file_path5, 'w') as f:\n",
    "            f.write(html5)\n",
    "    except ValueError:\n",
    "        print(f'inconsistent shapes {m}')\n",
    "visualize_clusters(korpus1_model, output_folder_name, \"korpus1\", korpus1_content, korpus1_text_source)#, korpus1_embeddings)\n",
    "visualize_clusters(korpus2_model, output_folder_name, \"korpus2\", korpus2_content, korpus2_text_source)#, korpus2_embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Topics in Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistent shapes korpus1\n"
     ]
    }
   ],
   "source": [
    "def visualize_clusters(model, output_folder_name, m, text_content, classes):\n",
    "    # Topics ordered by a specific class \n",
    "    try:\n",
    "        topic_distr, _ = model.approximate_distribution(text_content)\n",
    "        visualize_class = model.visualize_distribution(topic_distr[0])\n",
    "        html5 = pio.to_html(visualize_class)\n",
    "        html_file_path5 = os.path.join(output_folder_name, 'distribution' + '.html')\n",
    "        with open(html_file_path5, 'w') as f:\n",
    "            f.write(html5)\n",
    "    except ValueError:\n",
    "        print(f'inconsistent shapes {m}')\n",
    "visualize_clusters(korpus1_model, output_folder_name, \"korpus1\", korpus1_content, korpus1_text_source)#, korpus1_embeddings)\n",
    "visualize_clusters(korpus2_model, output_folder_name, \"korpus2\", korpus2_content, korpus2_text_source)#, korpus2_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba91e91136cc20f1f45937bee4ad571812535e64daee9a439fedd43c9cba856e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
