{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anmerkung\n",
    "Das Skript basiert auf Tim Feldmüllers Anwendung von TWEC, die unter dem folgenden GitLab-Link aufgerufen werden kann: https://gitlab.uzh.ch/zukoko/sommerschule-2023/-/tree/master/C5-Distributionelle-Semantik/TWEC_Clustering/scripts?ref_type=heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich zwischen Quellen\n",
    "Im unteren Beispiel kann eine .csv Datei eingelesen werden, welche die Texte nach der *text_source* sortiert. Je länger die Liste, desto länger dauert die Berechnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"./subcorpus_tp1.csv\"  # Update with the actual file path\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')\n",
    "\n",
    "# Define search words for each corpus as a list\n",
    "corpora = ['FAZ', 'BILD']  # Add as many as needed\n",
    "\n",
    "# Initialize a dictionary to store text content for each corpus\n",
    "sentences_corpus = {corpus: [] for corpus in corpora}\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    channel_id = row['text_source']\n",
    "    message = row['text_content']\n",
    "    \n",
    "    # Check and store messages for each corpus\n",
    "    for corpus in corpora:\n",
    "        if corpus in channel_id:\n",
    "            sentences_corpus[corpus].append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich zwischen Zeiträumen\n",
    "Im unteren Beispiel kann eine .csv Datei eingelesen werden, welche die Texte nach dem Metadatum *text_date* sortiert. Je länger die Liste, desto länger dauert die Berechnung. Man kann verschiedene Zeiträume setzen, welche die betreffende Fragestellung beleuchtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to parse dates\n",
    "def parse_date(date_str):\n",
    "    for fmt in ('%d-%m-%Y', '%Y-%m'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "# Define your time frames as a dictionary with corpus names as keys\n",
    "time_frames = {\n",
    "    'korpus1': (parse_date('01-01-2020'), parse_date('31-01-2020')),\n",
    "    'korpus2': (parse_date('01-09-2010'), parse_date('30-09-2010')),\n",
    "    'korpus3': (parse_date('01-09-1992'), parse_date('30-09-1992')),\n",
    "    # Add more time frames as needed\n",
    "}\n",
    "\n",
    "# Initialize your sentences_corpus dictionary\n",
    "sentences_corpus = {key: [] for key in time_frames.keys()}\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"./subcorpus_tp1.csv\"  # Update with the actual file path\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')  # Replace with your actual file path\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        channel_date = parse_date(row['text_date'])\n",
    "    except ValueError:\n",
    "        continue  # Skip rows with invalid date formats\n",
    "\n",
    "    message = row['text_content']\n",
    "\n",
    "    # Check each corpus defined in time_frames\n",
    "    for corpus_name, (start_date, end_date) in time_frames.items():\n",
    "        if start_date <= channel_date <= end_date:\n",
    "            sentences_corpus[corpus_name].append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to parse dates\n",
    "def parse_date(date_str):\n",
    "    for fmt in ('%d-%m-%Y', '%Y-%m'):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('no valid date format found')\n",
    "\n",
    "# Define your time frames as a dictionary with corpus names as keys\n",
    "time_frames = {\n",
    "    'korpus1': (parse_date('01-01-2020'), parse_date('31-01-2020')),\n",
    "    'korpus2': (parse_date('01-09-2010'), parse_date('30-09-2010')),\n",
    "    'korpus3': (parse_date('01-09-1992'), parse_date('30-09-1992')),\n",
    "    # Add more time frames as needed\n",
    "}\n",
    "\n",
    "# Initialize your sentences_corpus dictionary\n",
    "sentences_corpus = {key: [] for key in time_frames.keys()}\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = \"./subcorpus_tp1.csv\"  # Update with the actual file path\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv(csv_file_path, sep='\\t')  # Replace with your actual file path\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        channel_date = parse_date(row['text_date'])\n",
    "    except ValueError:\n",
    "        continue  # Skip rows with invalid date formats\n",
    "\n",
    "    message = row['text_content']\n",
    "\n",
    "    # Check each corpus defined in time_frames\n",
    "    for corpus_name, (start_date, end_date) in time_frames.items():\n",
    "        if start_date <= channel_date <= end_date:\n",
    "            sentences_corpus[corpus_name].append(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich zwischen Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Specify the path to your CSV file\n",
    "import os\n",
    "\n",
    "csv_file_name = \"telegram_data.csv\"  # Update with the actual file name\n",
    "csv_file_path = os.path.join(os.getcwd(), csv_file_name)\n",
    "\n",
    "# Define lists to store text content\n",
    "sentences_korpus1 = []\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    channel_id = row['channel_id']\n",
    "    message = row['message']\n",
    "    sentences_korpus1.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your CSV file\n",
    "\n",
    "csv_file_name = \"subcorpus_tp1.csv\"  # Update with the actual file name\n",
    "csv_file_path = os.path.join(os.getcwd(), csv_file_name)\n",
    "\n",
    "# Define lists to store text content\n",
    "sentences_korpus2 = []\n",
    "\n",
    "# Load the CSV file using pandas\n",
    "df = pd.read_csv(csv_file_path, sep='\\t', low_memory=False)\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    message = row['text_content']\n",
    "    sentences_korpus2.append(message)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Hier findet eine Tokenisierung der Daten mithilfe von *nltk* oder *spacy* statt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# Assuming you have a dictionary of sentences for each corpus\n",
    "#sentences_corpus = {'korpus1': ['hallo'], 'korpus2': ['hoi']}\n",
    "\n",
    "# Initialize a dictionary to store tokenized sentences for each corpus\n",
    "tokenized_corpus = {corpus: [] for corpus in sentences_corpus}\n",
    "\n",
    "# Tokenize sentences for each corpus\n",
    "for corpus, sentences in sentences_corpus.items():\n",
    "    for sent in sentences:\n",
    "        if type(sent) is str:\n",
    "            tokenized_corpus[corpus].append(word_tokenize(sent, language=\"german\"))\n",
    "# Save the tokenized sentences to separate files for each corpus\n",
    "for corpus, tokenized_sentences in tokenized_corpus.items():\n",
    "    with open(f\"./data/sentences_{corpus}.txt\", \"w\") as f:\n",
    "        for sent in tokenized_sentences:\n",
    "            f.write(\" \".join(sent) + \"\\n\")\n",
    "\n",
    "# Optionally, save all tokenized sentences in a single file\n",
    "with open(\"./data/all_sentences.txt\", \"w\") as f:\n",
    "    for tokenized_sentences in tokenized_corpus.values():\n",
    "        for sent in tokenized_sentences:\n",
    "            f.write(\" \".join(sent) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the German language model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Assuming you have a dictionary of sentences for each corpus\n",
    "# sentences_corpus = {'korpus1': [...], 'korpus2': [...], ...}\n",
    "\n",
    "# Initialize a dictionary to store tokenized sentences for each corpus\n",
    "tokenized_corpus = {corpus: [] for corpus in sentences_corpus}\n",
    "\n",
    "# Tokenize sentences for each corpus using spaCy\n",
    "for corpus, sentences in sentences_corpus.items():\n",
    "    for sent in sentences:\n",
    "        if type(sent) is str:\n",
    "            doc = nlp(sent)\n",
    "            tokenized_sentence = [token.text for token in doc]\n",
    "            tokenized_corpus[corpus].append(tokenized_sentence)\n",
    "\n",
    "# Save the tokenized sentences to separate files for each corpus\n",
    "for corpus, tokenized_sentences in tokenized_corpus.items():\n",
    "    with open(f\"../data/sentences_{corpus}.txt\", \"w\") as f:\n",
    "        for sent in tokenized_sentences:\n",
    "            f.write(\" \".join(sent) + \"\\n\")\n",
    "\n",
    "# Optionally, save all tokenized sentences in a single file\n",
    "with open(\"../data/all_sentences.txt\", \"w\") as f:\n",
    "    for tokenized_sentences in tokenized_corpus.values():\n",
    "        for sent in tokenized_sentences:\n",
    "            f.write(\" \".join(sent) + \"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary mit Frequenzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eine Hilfsfunktion, die ein Dictionary nach seinen Werten sortiert\n",
    "def sort_dict(dic, reverse=True):\n",
    "    return dict(sorted(dic.items(), key=lambda item: item[1], reverse=reverse))\n",
    "\n",
    "\n",
    "# Mit dieser Funktion können wir eine für TWEC aufbereitete Textdatei einlesen, die enthaltenen Types auszählen lassen und bekommen ein nach Frequenz sortiertes\n",
    "# Dictionary zurück\n",
    "def get_word_frequencies(path_to_txt):\n",
    "    frequencies = {}\n",
    "\n",
    "    with open(path_to_txt, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            for word in line.split():\n",
    "                if word in frequencies:\n",
    "                    frequencies[word] += 1\n",
    "                else:\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "    return sort_dict(frequencies)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a function get_word_frequencies(file_path) defined\n",
    "\n",
    "# List of corpus names\n",
    "# Initialize a dictionary to store frequency dictionaries for each corpus\n",
    "freqs = {}\n",
    "\n",
    "# Create frequency dictionaries for each corpus\n",
    "\n",
    "for corpus, sentences in sentences_corpus.items():\n",
    "    file_path = f\"./data/sentences_{corpus}.txt\"\n",
    "    freqs[corpus] = get_word_frequencies(file_path)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Initialisieren von TWEC passiert hier. Wir legen fest, wie viele Dimensionen unsere Word Embeddings haben sollen (size) -> 200–300 sind übliche Werte wie frequent sie mindestens sein müssen (min_count) -> hier nur auf 3 gesetzt, da das Testkorpus sehr klein ist. Es empfehlen sich generell eher höhere Werte, da für sehr niedrigfrequente Wörter nicht genug Informationen erlernbar sind, um sie gut im Vektorraum abbilden zu können wie viele Wörter links und rechts jedes Wortes werden berücksichtigt beim Training (window) -> ähnlich einem Kollokationsfenster, 5 – 6 sind übliche Werte wie viele CPU-Kerne sollen genutzt werden für das Training -> hier automatisch bestimmt (einer weniger als verfügbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twec.twec import TWEC\n",
    "import multiprocessing\n",
    "\n",
    "min_count = 5\n",
    "aligner = TWEC(\n",
    "    size=300, min_count=min_count, window=6, workers=multiprocessing.cpu_count() - 1\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun trainieren wir den sogenannten Kompass auf dem Gesamtkorpus. Dieses Kompassmodell dient danach dazu, auch die Vektorräume von Teilkorpora auf die gleiche Art ausrichten zu können, sodass alle Modelle den gleichen Vektorraum nutzen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.train_compass(\"./data/all_sentences.txt\", overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Training ist fertig und wir können das fertige Modell einlesen\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "compass = Word2Vec.load(\"model/compass.model\")\n",
    "\n",
    "# ... und testweise einen Vektor abfragen\n",
    "compass.wv.most_similar(\"Volk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt geht es an das Training der Subkorpus-Modelle bzw. 'Slices'\n",
    "# Mit dem For-Loop können wir das Training durchführen, wir müssen nur darauf achten,\n",
    "# dass wir die Textdateien für das Training nach dem Muster sentences_METADATUM.txt benannt haben\n",
    "# und die Metadaten in der Liste slices genauso aufführen\n",
    "# Es passiert außerdem auch ein weiterer Schritt: Und zwar wird von TWEC an dieser Stelle, anders als beim Training des Kompass,\n",
    "# keine Filterung nach dem Parameter min_count mehr vorgenommen. Daher erstellen wir die TXT Files neu, speichern sie mit dem Suffix _min_count\n",
    "# und lassen alle Wörter aus, die wir nicht mit einer Mindestfrequenz in unseren Frequency Dictionaries finden\n",
    "slices = []\n",
    "for corpus, i in sentences_corpus.items():\n",
    "    slices.append(corpus)\n",
    "for slice in slices:\n",
    "    path_input = f\"./data/sentences_{slice}.txt\"\n",
    "    path_output = path_input.replace(\".txt\", \"_min_count.txt\")\n",
    "    with open(path_input, \"r\") as f:\n",
    "        with open(path_output, \"a\") as f_out:\n",
    "            for sentence in f.readlines():\n",
    "                min_count_tokens = []\n",
    "                for word in sentence.split():\n",
    "                    if freqs[slice][word] >= min_count:\n",
    "                        min_count_tokens.append(word)\n",
    "                f_out.write(\" \".join(min_count_tokens) + \"\\n\")\n",
    "\n",
    "    aligner.train_slice(path_output, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichern der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy\n",
    "\n",
    "model_korpus1 = Word2Vec.load(\"./model/sentences_korpus1_min_count.model\")\n",
    "model_korpus2 = Word2Vec.load(\"./model/sentences_korpus2_min_count.model\")\n",
    "model_korpus3 = Word2Vec.load(\"./model/sentences_korpus3_min_count.model\")\n",
    "# weitere Slices können hinzugefügt werden\n",
    "\"\"\"\n",
    "model_korpusX = Word2Vec.load(\"./model/sentences_korpusX_min_count.model\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jetzt können wir diesen Vektor in den korpus-Modellen abfragen\n",
    "model_korpus1.wv.most_similar(['Volk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_korpus3.wv.most_similar(['Volk'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleiche"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Funktion time_machine() können wir diese Schritte bündeln und z.B. abfragen, welches Wort in einem Korpus (Zeit oder Medium) äquivalent zu einem Wort in einem anderen Korpus ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "lib.time_machine(model_korpus1, model_korpus3, \"Volk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "lib.time_machine(model_korpus3, model_korpus1, \"Volk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesem Befehl können wir alle einzelnen Vektoren unseres Modells als Liste abfragen und sie in der Variable word_vectors speichern.\n",
    "\n",
    "Bei anderen Versionen von Word2Vec kann der Befehl auch model.wv[model.wv.key_to_index] lauten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model_korpus1.wv.vocab.keys()\n",
    "word_vectors = model_korpus1.wv[words]\n",
    "print(\"Anzahl der zu clusternden Vektoren:\", len(word_vectors))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Zelle dient nur dazu, ein Sample aus den Vektoren zu erstellen, damit die Berechnung nicht zu lange dauert.\n",
    "Wir nutzen dafür nochmal unsere Funktion get_word_frequencies() und berechnen dann nur Cluster für alle\n",
    "- Vektoren mit einer Häufigkeit >= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import get_word_frequencies\n",
    "\n",
    "freqs = {}\n",
    "freqs[\"korpus1\"] = get_word_frequencies(\"./data/sentences_korpus1.txt\")\n",
    "\n",
    "min_count = 30\n",
    "\n",
    "words = []\n",
    "for word in model_korpus1.wv.vocab.keys():\n",
    "    if freqs[\"korpus1\"][word] >= 80:\n",
    "        words.append(word)\n",
    "\n",
    "word_vectors = model_korpus1.wv[words]\n",
    "\n",
    "print(len(word_vectors))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anzahl der Cluster k muss festgelegt werden. Wir können verschiedene Einstellungen ausprobieren\n",
    "oder die Anzahl der Cluster nach der Anzahl der Types in unserem Korpus richten, wie hier können wir verschiedene Werte ausprobieren \n",
    "- finegranular vs. big picture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(word_vectors) * 0.024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=k) #k\n",
    "kmeans.fit(word_vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sind in der gleichen Reihenfolge gelistet wie die übergebenen Vektoren in unserer Variable words und wir können sie mit zip() zu einem dictionary zusammenfassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster = dict(zip(words, kmeans.labels_))\n",
    "print(word_cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir speichern die Zuordnung der Cluster zu ihren Mittelpunkten (Centroids) im Vektorraum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroid = dict(\n",
    "    zip(range(len(kmeans.cluster_centers_)), kmeans.cluster_centers_)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir unsere Daten in einer tabellarischen Struktur zusammenbringen. Dafür vermerken wir für jedes Wort sein Cluster in einer Spalte und in einer weiteren Spalte berechnen wir für jedes Wort seine Cosinusdistanz (bzw. Cosinus-Ähnlichkeit) zum Mittelpunkt des Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df = pd.DataFrame(word_cluster.items(), columns=[\"word\", \"cluster\"])\n",
    "df[\"sim\"] = [\n",
    "    1 - distance.cosine(model_korpus1[word], cluster_centroid[word_cluster[word]])\n",
    "    for word in df[\"word\"]\n",
    "]\n",
    "df = df.sort_values(by=[\"cluster\", \"sim\"], ascending=[True, False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So sieht das Ergebnis aus. mit pd.to_csv(\"datei/pfad.csv\") könnten wir die Tabelle so sichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)\n",
    "df.to_csv(\"cluster.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine bessere Übersicht möchten wir noch eine Zuordnung der Cluster-IDs zu den enthaltenen Wörtern in einem dictionary haben. Die Wörter sind dabei nach ihrer Cosinus-Ähnlichkeit zum jeweiligen Centroid sortiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if row[\"cluster\"] in cluster_words:\n",
    "        cluster_words[row[\"cluster\"]].append(row[\"word\"])\n",
    "    else:\n",
    "        cluster_words[row[\"cluster\"]] = [row[\"word\"]]\n",
    "\n",
    "print(cluster_words[1])\n",
    "print(cluster_words[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und außerdem brauchen wir statt der numerischen IDs interpretierbare Labels Dazu erstellen wir eine Zusammenstellung von Cluster-ID zu einem Label, das aus den drei zentralsten Wörtern des Clusters, also den ersten drei Wörtern der Liste, besteht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "cluster_label = {}\n",
    "\n",
    "for cluster, words in cluster_words.items():\n",
    "    cluster_label[cluster] = \"|\".join(words[:10])\n",
    "cluster_label\n",
    "\"\"\"\n",
    "# Specify the CSV file name\n",
    "csv_file = 'data.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = cluster_label[0].keys()  # Use the keys from the dictionary as column headers\n",
    "\n",
    "    # Create a DictWriter object\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Write the data\n",
    "    writer.writerows(cluster_label)\n",
    "\n",
    "print(f'Data has been written to {csv_file}')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möchten wir nun z.B anhand eines Wortes wissen, mit welchen anderen Wörtern es ein Cluster teilt, können wir das wie folgt abfragen word_cluste[\"korpus2\"] gibt uns die Cluster-ID zurück, die wir dann im dicionary cluster_words abfragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_words[word_cluster[\"Volk\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion berechnet das Modell. Wir übergeben den Pfad zum Word-Embedding-Modell, das geclustert werden soll, die Anzahl der Cluster k sowie optional einen Pfad zum Sichern des fertigen kmeans-Modells. Wird hier kein Pfad übergeben, wird das Modell im gleichen Ordner wie das WE-Modell gesichert. Außerdem können wir optional eine Mindestfrequenz für die Types des Korpus sowie ein entsprechendes Dictionary, in dem diese ausgezählt sind, übergeben. Ein Funktionsaufruf zum trainieren unseres obigen Modells  könnte so aussehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "from sklearn import cluster\n",
    "\n",
    "kmeans = lib.compute_kmeans(\"model/sentences_korpus2_min_count.model\", k=k, min_count=100)\n",
    "kmeans2 = lib.compute_kmeans(\"model/sentences_korpus1_min_count.model\", k=k, min_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster, cluster_words, cluster_centroid, cluster_label = lib.load_kmeans(\"model/sentences_korpus2_min_count.model.kmeans.pkl\", \"model/sentences_korpus2_min_count.model\")\n",
    "word_cluster2, cluster_words2, cluster_centroid2, cluster_label2 = lib.load_kmeans(\"model/sentences_korpus1_min_count.model.kmeans.pkl\", \"model/sentences_korpus1_min_count.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So wie wir zu einzelnen Wörtern nächste Nachbarn abfragen können, können wir das auch zu den Clustern, indem wir den Centroid der Cluster verwenden dafür modifizieren wir unsere time_machine Funktion, sodass wir ihr entweder ein Wort oder einen Vektor übergeben können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_machine(model1, model2, word_or_vec):\n",
    "    if type(word_or_vec) == str:\n",
    "        embedding = model1[word_or_vec]\n",
    "    else:\n",
    "        embedding = word_or_vec\n",
    "    return model2.wv.most_similar([embedding])\n",
    "\n",
    "centroid = cluster_centroid2[word_cluster2[\"Solidarität\"]]\n",
    "time_machine(model_korpus1, model_korpus2, centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = cluster_centroid[word_cluster[\"Solidarität\"]]\n",
    "time_machine(model_korpus2, model_korpus1, centroid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ähnliche Cluster berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.get_most_similar_clusters(model_korpus1[\"Solidarität\"], cluster_centroid2, cluster_label2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entfernte Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "lib.get_most_distant_clusters(\n",
    "    model_korpus1, cluster_words, cluster_label, model_korpus2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantische Achse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "antonym_list = [\n",
    "(\"gut\", \"schlecht\"),\n",
    "]\n",
    "\n",
    "word_list = ['gut', 'schlecht']\n",
    "\n",
    "cos_sim = lib.we_basics('./model/sentences_korpus1_min_count.model', antonym_list, word_list)\n",
    "lib.plot_cosine_similarity(cos_sim, antonym_list, word_list)\n",
    "cos_sim = lib.we_basics('./model/sentences_korpus3_min_count.model', antonym_list, word_list)\n",
    "lib.plot_cosine_similarity(cos_sim, antonym_list, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "antonym_list = [\n",
    "(\"gut\", \"schlecht\"),\n",
    "]\n",
    "\n",
    "antonym_list_2 = [\n",
    "('Liebe', 'Hass')\n",
    "    # ... (Other antonym pairs)\n",
    "]\n",
    "word_list = ['Solidarität']\n",
    "cos_sim = lib.we_basics('./model/compass.model', antonym_list, word_list)\n",
    "lib.plot_cosine_similarity2(cos_sim, cos_sim, antonym_list=antonym_list, antonym_list2=antonym_list_2, word_list=word_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twec_kernel",
   "language": "python",
   "name": "twec_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c181cd67bddd6f6971fd187b50a9faa0c657508828b23bbaab2a73d2f442461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
